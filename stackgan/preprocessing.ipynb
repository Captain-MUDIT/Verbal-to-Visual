{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load the Flowers BLIP Captions dataset\ndataset = load_dataset(\"pranked03/flowers-blip-captions\")\n\n# Save the dataset to your local device\ndataset.save_to_disk(\"flowers-blip-captions\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_from_disk\n\n\ndataset = load_from_disk(\"flowers-blip-captions\")\nprint(dataset['train'][0]) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom tqdm import tqdm  \\\nfrom datasets import load_from_disk\n\n\ndataset = load_from_disk(\"flowers-blip-captions\")\nimage_output_dir = 'extracted_images_5'\ntext_output_dir = 'extracted_text_5'\nos.makedirs(image_output_dir, exist_ok=True)\nos.makedirs(text_output_dir, exist_ok=True)\n\ndef save_images_and_text():\n    for index in tqdm(range(len(dataset['train'])), desc=\"Extracting Images and Text\"): \n        example = dataset['train'][index]  \n\n        label = example['label']\n        \n        image = example['image']  \n        image.save(os.path.join(image_output_dir, f'image_{index}.png'))\n\n   \n        caption = example['text']  \n        with open(os.path.join(text_output_dir, f'image_{index}.txt'), 'w') as file:\n            file.write(caption)\n\nsave_images_and_text()\n\nprint(f\"Images saved to {image_output_dir} and text files saved to {text_output_dir}.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom tqdm import tqdm  \nfrom transformers import BertTokenizer, BertModel\nimport torch\n\ntext_output_dir = 'extracted_text_5'\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\n# Dictionary to store text embeddings\nembeddings_dict = {}\n\ndef create_text_embeddings(\n  \n    text_files = [f for f in os.listdir(text_output_dir) if f.endswith('.txt')]\n    \n    for text_filename in tqdm(text_files, desc=\"Extracting Text Embeddings\"):\n        text_path = os.path.join(text_output_dir, text_filename)\n        with open(text_path, 'r') as file:\n            caption = file.read().strip()  \n\n        inputs = tokenizer(caption, return_tensors='pt', padding=True, truncation=True)\n        with torch.no_grad():\n            outputs = model(**inputs)\n            embedding = outputs.last_hidden_state[:, 0, :]\n\n        image_index = text_filename.replace('.txt', '')\n\n        embeddings_dict[image_index] = embedding\ncreate_text_embeddings()\n\nprint(\"Text embeddings processed and stored.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for extracting 64*64 images\nimport torchvision.transforms as transforms\nimport random\nfrom PIL import Image\nimage_output_dir = 'extracted_images_5'\n\nimage_files = [f for f in os.listdir(image_output_dir) if f.endswith('.png')]\n\nrandom.shuffle(image_files)\n\ntransform = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.ToTensor(),  # Convert PIL image to tensor\n    transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])  # Normalize with ImageNet stats\n])\nimage_tensor_dict = {}\ndef convert_images_to_tensors():\n    image_files = [f for f in os.listdir(image_output_dir) if f.endswith('.png')]  # Get all image files\n    \n    for image_file in tqdm(image_files, desc=\"Converting Images to Tensors\"):\n        image_path = os.path.join(image_output_dir, image_file)  # Full path to the image\n        image = Image.open(image_path)  # Open the image\n        \n        # Convert image to tensor and normalize\n        image_tensor = transform(image)  # Apply the transform\n        \n        # Add tensor to dictionary with key as image name without '.png'\n        image_index = image_file.split('.')[0]\n\n        image_tensor_dict[image_index] = image_tensor\n        \nconvert_images_to_tensors()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(image_tensor_dict))\n\nordered_embeddings_dict = {}\nfor key in image_tensor_dict.keys():\n    # image_tensor_dict[key]\n    if key in embeddings_dict:\n        ordered_embeddings_dict[key] = embeddings_dict[key]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for extracting 256*256 imagees\nimport torchvision.transforms as transforms\nimport random\nfrom PIL import Image\nimage_output_dir = 'extracted_images'\n\n\nimage_files = [f for f in os.listdir(image_output_dir) if f.endswith('.png')]\n\nrandom.shuffle(image_files)\n\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),  \n    transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])  \n])\nimage_tensor_dict = {}\ndef convert_images_to_tensors():\n    image_files = [f for f in os.listdir(image_output_dir) if f.endswith('.png')] \n    \n    for image_file in tqdm(image_files, desc=\"Converting Images to Tensors\"):\n        image_path = os.path.join(image_output_dir, image_file)  \n        image = Image.open(image_path)  \n        \n        image_tensor = transform(image) \n        image_index = image_file.split('.')[0]\n\n        image_tensor_dict[image_index] = image_tensor\n        \nconvert_images_to_tensors()\n\n# arrange this dictionary inorder of the embedding dictionary","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}