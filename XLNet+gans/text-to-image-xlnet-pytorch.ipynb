{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":537279,"sourceType":"datasetVersion","datasetId":255782},{"sourceId":67010937,"sourceType":"kernelVersion"}],"dockerImageVersionId":30097,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport io\nimport h5py\nimport torch\nimport ipywidgets\nimport numpy as np\nfrom torch import nn\nfrom PIL import Image\nfrom IPython import display\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom transformers import XLNetTokenizer, XLNetModel\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom sklearn.model_selection import StratifiedShuffleSplit","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-17T10:52:10.775304Z","iopub.execute_input":"2024-10-17T10:52:10.775651Z","iopub.status.idle":"2024-10-17T10:52:10.782171Z","shell.execute_reply.started":"2024-10-17T10:52:10.775622Z","shell.execute_reply":"2024-10-17T10:52:10.781019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training = 1\nnum_epoch = 25\nbatch_size = 32\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndataset_root = os.path.join(\"/\", \"kaggle\", \"input\", \"flowershd5dataset\")\nkernel_root = os.path.join(\"/\", \"kaggle\", \"input\", \"text-to-image-xlnet-pytorch\")","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:52:12.414275Z","iopub.execute_input":"2024-10-17T10:52:12.414637Z","iopub.status.idle":"2024-10-17T10:52:12.490233Z","shell.execute_reply.started":"2024-10-17T10:52:12.414602Z","shell.execute_reply":"2024-10-17T10:52:12.489137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TextEncoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.transformer = XLNetModel.from_pretrained(\"xlnet-base-cased\")\n\n    def forward(self, input_ids, token_type_ids, attention_mask):\n        hidden = self.transformer(\n            input_ids=input_ids,\n            token_type_ids=token_type_ids,\n            attention_mask=attention_mask\n        ).last_hidden_state\n        context = hidden.mean(dim=1)\n        context = context.view(*context.shape, 1, 1)\n        return context","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:52:13.87497Z","iopub.execute_input":"2024-10-17T10:52:13.875348Z","iopub.status.idle":"2024-10-17T10:52:13.88149Z","shell.execute_reply.started":"2024-10-17T10:52:13.875317Z","shell.execute_reply":"2024-10-17T10:52:13.880475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, nz=100, nt=768, nc=3, ngf=64):\n        super().__init__()\n        \n        self.layer1 = nn.Sequential(\n            # input is Z, going into a convolution\n            nn.ConvTranspose2d(nz + nt, ngf * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 8),\n            # nn.ReLU(True),\n            # state size. (ngf*8) x 4 x 4\n        )\n\n            # Completed - TODO: check out paper's code and add layers if required\n\n            ##there are more conv2d layers involved here in \n            # https://github.com/reedscot/icml2016/blob/master/main_cls.lua\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(ngf*8,ngf*2,1,1),\n            nn.Dropout2d(inplace=True),            \n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            # nn.SELU(True),\n        )\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(ngf*2,ngf*2,3,1,1),\n            nn.Dropout2d(inplace=True),            \n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            # nn.SELU(True),\n        )\n        self.layer4 = nn.Sequential(\n            nn.Conv2d(ngf*2,ngf*8,3,1,1),\n            nn.Dropout2d(inplace=True),            \n            nn.BatchNorm2d(ngf * 8),\n            nn.ReLU(inplace=True),\n            # nn.SELU(True),\n        )\n        self.layer5 = nn.Sequential(\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),   \n            nn.BatchNorm2d(ngf * 4),\n            # nn.ReLU(True),\n            # state size. (ngf*4) x 8 x 8\n        )\n            \n            # Completed - TODO: check out paper's code and add layers if required\n            \n            ##there are more conv2d layers involved here in \n            # https://github.com/reedscot/icml2016/blob/master/main_cls.lua\n            \n        self.layer6 = nn.Sequential(\n            nn.Conv2d(ngf*4,ngf,1,1),\n            nn.Dropout2d(inplace=True),            \n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            # nn.SELU(True),\n        )\n        self.layer7 = nn.Sequential(\n            nn.Conv2d(ngf,ngf,3,1,1),\n            nn.Dropout2d(inplace=True),            \n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            # nn.SELU(True),\n        )\n        self.layer8 = nn.Sequential(\n            nn.Conv2d(ngf,ngf*4,3,1,1),\n            nn.Dropout2d(inplace=True),            \n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            # nn.SELU(True),\n        )\n        self.layer9 = nn.Sequential(  \n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            # nn.SELU(True),\n            \n            # state size. (ngf*2) x 16 x 16\n        )\n        self.layer10 = nn.Sequential(\n            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            # nn.SELU(True),\n\n            # state size. (ngf) x 32 x 32\n        )\n        self.layer11 = nn.Sequential(\n            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n            # state size. (nc) x 64 x 64\n        )\n  \n    def forward(self,noise,encoded_text):\n        x = torch.cat([noise,encoded_text],dim=1)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.layer5(x)\n        x = self.layer6(x)\n        x = self.layer7(x)\n        x = self.layer8(x)\n        x = self.layer9(x)\n        x = self.layer10(x)\n        x = self.layer11(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:52:15.411371Z","iopub.execute_input":"2024-10-17T10:52:15.411752Z","iopub.status.idle":"2024-10-17T10:52:15.431882Z","shell.execute_reply.started":"2024-10-17T10:52:15.411722Z","shell.execute_reply":"2024-10-17T10:52:15.430912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, nc=3, ndf=64, nt=768):\n        super().__init__()\n        self.layer1 = nn.Sequential(\n            # input is (nc) x 64 x 64\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n        self.layer2 = nn.Sequential(\n            # state size. (ndf) x 32 x 32\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n        self.layer3 = nn.Sequential(\n            # state size. (ndf*2) x 16 x 16\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n        self.layer4 = nn.Sequential(\n            # state size. (ndf*4) x 8 x 8\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n\n            nn.Conv2d(ndf*8,ndf*2,1,1),\n            # nn.Dropout2d(inplace=True),            \n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n        self.layer5 = nn.Sequential(\n\n            nn.Conv2d(ndf*2,ndf*2,3,1,1),\n            # nn.Dropout2d(inplace=True),            \n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n        self.layer6 = nn.Sequential(\n\n            nn.Conv2d(ndf*2,ndf*8,3,1,1),\n            # nn.Dropout2d(inplace=True),            \n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True)\n        )\n        self.concat_image_n_text = nn.Sequential(\n            nn.Conv2d(ndf * 8 + nt, ndf * 8, 1, 1, 0, bias=False), ## TODO: Might want to change the kernel size and stride\n            nn.BatchNorm2d(ndf*8),\n            nn.LeakyReLU(0.2,inplace=True),\n            nn.Conv2d(ndf * 8, 2, 4, 1, 0, bias=False),\n            nn.Flatten(start_dim=1)\n        )\n\n    def forward(self, x, encoded_text):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.layer5(x)\n        x = self.layer6(x)\n#         print(x.shape)\n        x = torch.cat([x, encoded_text.repeat(1, 1, 4, 4)], dim=1)\n        x = self.concat_image_n_text(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:52:16.695584Z","iopub.execute_input":"2024-10-17T10:52:16.69597Z","iopub.status.idle":"2024-10-17T10:52:16.712674Z","shell.execute_reply.started":"2024-10-17T10:52:16.695917Z","shell.execute_reply":"2024-10-17T10:52:16.711557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainDataset:\n    def __init__(self, dataset_root, kernel_root):\n        self.tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n\n        if os.path.exists(os.path.join(kernel_root, \"data.npy\")):\n            self.data = np.load(os.path.join(kernel_root, \"data.npy\"), allow_pickle=True)\n        else:\n            f = h5py.File(os.path.join(dataset_root, \"data\", \"flowers\", \"flowers.hdf5\"), mode=\"r\")\n            self.data = self.prepareData(f['train'])\n        np.save('data.npy', self.data)\n        self.max_seq_len = max(map(lambda x: len(x[\"text\"][\"input_ids\"]), self.data))\n\n        self.transforms = transforms.Compose([\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomRotation(degrees=(90, 90)),\n            transforms.RandomRotation(degrees=(180, 180)),\n            transforms.RandomRotation(degrees=(270, 270)),\n            transforms.RandomVerticalFlip(p=1),\n            transforms.Resize((64, 64)),\n            transforms.ToTensor(),\n        ])\n\n    def prepareData(self, data):\n        preparedData = []\n        for idx, img_name in enumerate(tqdm(data)):\n            image = np.array(Image.open(io.BytesIO(bytes(np.array(data[img_name]['img'])))).resize((256,256)))\n            text = np.array(data[img_name]['txt']).item().strip()\n            input_ids = self.tokenizer.encode(text)\n            token_type_ids = [0] * (len(input_ids) - 1) + [1]\n            attention_mask = [1] * len(token_type_ids)\n            preparedData.append({\n                \"image\": image,\n                \"text\": {\n                    \"input_ids\": input_ids,\n                    \"token_type_ids\": token_type_ids,\n                    \"attention_mask\": attention_mask\n                },\n            })\n        return preparedData\n\n    def padTokens(self, text_dict):\n        pad_len = self.max_seq_len - sum(text_dict[\"attention_mask\"])\n        text_dict['input_ids'] =  [5] * pad_len + text_dict['input_ids'] # <pad> = 5\n        text_dict['token_type_ids'] =  [2] * pad_len + text_dict['token_type_ids']\n        text_dict['attention_mask'] = [0] * pad_len + text_dict['attention_mask']   \n        return text_dict\n\n    @staticmethod\n    def collate_fn_module(batch, idx):\n        images, texts = [], {}\n        for data in batch:\n            images.append(data[idx][0])\n            for key in data[idx][1]:\n                if key not in texts:\n                    texts[key] = []\n                texts[key].append(data[0][1][key])\n\n        images = torch.stack(images).to(device)\n        for key in texts:\n            texts[key] = torch.tensor(texts[key]).to(device)\n        return images, texts\n\n    def collate_fn(self, batch):\n        right_images, right_texts = self.collate_fn_module(batch, 0)\n        wrong_images, wrong_texts = self.collate_fn_module(batch, 1)\n        return (right_images, right_texts), (wrong_images, wrong_texts)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, right_idx):\n        right_data = self.data[right_idx].copy()\n        right_image = self.transforms(Image.fromarray(right_data[\"image\"]))\n        right_text = self.padTokens(right_data[\"text\"].copy())\n\n        wrong_idx = np.random.choice([(i) for i in range(len(self.data)) if i != right_idx])\n        wrong_data = self.data[wrong_idx].copy()\n        wrong_image = self.transforms(Image.fromarray(wrong_data[\"image\"]))\n        wrong_text = self.padTokens(wrong_data[\"text\"].copy())\n        return (right_image, right_text), (wrong_image, wrong_text)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:52:18.121817Z","iopub.execute_input":"2024-10-17T10:52:18.122321Z","iopub.status.idle":"2024-10-17T10:52:18.152337Z","shell.execute_reply.started":"2024-10-17T10:52:18.122275Z","shell.execute_reply":"2024-10-17T10:52:18.151037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_dataset = TrainDataset(dataset_root, kernel_root)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:52:19.704498Z","iopub.execute_input":"2024-10-17T10:52:19.704901Z","iopub.status.idle":"2024-10-17T10:53:19.092888Z","shell.execute_reply.started":"2024-10-17T10:52:19.704866Z","shell.execute_reply":"2024-10-17T10:53:19.091698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize(image, text, tokenizer=None, ax = None, title = None):\n    if ax is None: _, ax = plt.subplots(1, 1, figsize = (10, 10))\n    if title: ax.set_title(title)\n    ax.imshow(image.permute(1, 2, 0))\n    if tokenizer== None:\n        pass\n    else:\n        ax.set_xlabel(tokenizer.decode([t for t in text[\"input_ids\"] if t != 5]))","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:53:21.674512Z","iopub.execute_input":"2024-10-17T10:53:21.674875Z","iopub.status.idle":"2024-10-17T10:53:21.687369Z","shell.execute_reply.started":"2024-10-17T10:53:21.67484Z","shell.execute_reply":"2024-10-17T10:53:21.686458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx = np.random.randint(len(train_dataset))\nright, wrong = train_dataset[idx]\n_, axs = plt.subplots(1, 2, figsize=(5, 5))\nvisualize(*right, train_dataset.tokenizer, axs[0], \"Right\")\nvisualize(*wrong, ax = axs[1], title = \"Wrong\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:53:23.88109Z","iopub.execute_input":"2024-10-17T10:53:23.88157Z","iopub.status.idle":"2024-10-17T10:53:28.965354Z","shell.execute_reply.started":"2024-10-17T10:53:23.881517Z","shell.execute_reply":"2024-10-17T10:53:28.964383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if os.path.exists(os.path.join(kernel_root, 'checkpoint.pth')):\n    checkpoint = torch.load(os.path.join(kernel_root, 'checkpoint.pth'), map_location=device)\n    torch.save(checkpoint, \"checkpoint.pth\")\nelse:\n    checkpoint = {}","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:53:36.570598Z","iopub.execute_input":"2024-10-17T10:53:36.571005Z","iopub.status.idle":"2024-10-17T10:53:48.959482Z","shell.execute_reply.started":"2024-10-17T10:53:36.570967Z","shell.execute_reply":"2024-10-17T10:53:48.958392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_indices = checkpoint.get('train_indices', None)\nvalid_indices = checkpoint.get('valid_indices', None)\nif train_indices is None or valid_indices is None:\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\n    splitter = sss.split(train_dataset, np.array([len(x[\"text\"][\"input_ids\"])%10 for x in train_dataset.data]))\n    train_indices, valid_indices = next(splitter)\n# Creating PT data samplers and loaders\ntrain_sampler = SubsetRandomSampler(train_indices)\nvalid_sampler = SubsetRandomSampler(valid_indices)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, collate_fn=train_dataset.collate_fn, drop_last=True)\nvalid_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=valid_sampler, collate_fn=train_dataset.collate_fn, drop_last=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:53:50.850911Z","iopub.execute_input":"2024-10-17T10:53:50.851305Z","iopub.status.idle":"2024-10-17T10:53:50.85933Z","shell.execute_reply.started":"2024-10-17T10:53:50.851268Z","shell.execute_reply":"2024-10-17T10:53:50.858311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = {\n    \"text_encoder\": TextEncoder().to(device),\n    \"generator\": Generator().to(device),\n    \"discriminator\": Discriminator().to(device)\n}\noptimizers = {}\nschedulers = {}\ncriterion = nn.CrossEntropyLoss()\nlearning_rates = {\"text_encoder\": 1e-4, \"generator\": 5e-4, \"discriminator\": 1e-4}\nfor key in models:\n    if key == \"text_encoder\": continue\n    models[key].load_state_dict(checkpoint.get(\"models\", {}).get(key, models[key].state_dict()))\n    # Prepare optimizer and schedule (linear warmup and decay)\n    optimizers[key] = torch.optim.Adam(models[key].parameters(), lr=learning_rates[key], weight_decay=1e-6)\n    optimizers[key].load_state_dict(checkpoint.get(\"optimizers\", {}).get(key, optimizers[key].state_dict()))\n    schedulers[key] = torch.optim.lr_scheduler.StepLR(optimizers[key], step_size=50, gamma=0.1, last_epoch=-1)\n    schedulers[key].load_state_dict(checkpoint.get(\"schedulers\", {}).get(key, schedulers[key].state_dict()))","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:53:52.015378Z","iopub.execute_input":"2024-10-17T10:53:52.015712Z","iopub.status.idle":"2024-10-17T10:54:07.096585Z","shell.execute_reply.started":"2024-10-17T10:53:52.015683Z","shell.execute_reply":"2024-10-17T10:54:07.095724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def valid():\n    total_dis_loss, total_gen_loss = 0, 0\n    total_dis_score, total_gen_score = 0, 0\n    template = \"Dis Loss: {:.4f}, Dis Score: {:.4f}, Gen Loss: {:.4f}, Gen Score: {:.4f}\"\n    for key in models:\n        models[key].eval()\n    loader = tqdm(valid_loader, desc = f\"Validating\")\n    for idx, ((right_images, right_texts), (wrong_images, wrong_texts)) in enumerate(loader, start=1):\n        # Execute\n        with torch.no_grad():\n            enc_right_texts = models[\"text_encoder\"](**right_texts).detach()\n            enc_wrong_texts = models[\"text_encoder\"](**wrong_texts).detach()\n\n            noise = torch.randn((len(right_images), 100, 1, 1),device = device)\n            ones = torch.ones((len(right_images)), dtype=torch.long, device=device)\n            zeros = torch.zeros((len(right_images)), dtype=torch.long, device=device)\n            fake_images = models[\"generator\"](noise, enc_right_texts)\n\n            dis_right_logits = models[\"discriminator\"](right_images, enc_right_texts)\n            dis_wrong_text_logits =  models[\"discriminator\"](right_images, enc_wrong_texts)\n            dis_wrong_image_logits =  models[\"discriminator\"](wrong_images, enc_right_texts)\n            dis_fake_logits = models[\"discriminator\"](fake_images.detach(), enc_right_texts)\n\n            dis_real_loss = criterion(dis_right_logits, ones)\n            dis_wrong_text_loss = criterion(dis_wrong_text_logits, zeros)\n            dis_wrong_image_loss = criterion(dis_wrong_image_logits, zeros)\n            dis_fake_loss = criterion(dis_fake_logits, zeros)\n            dis_loss = dis_real_loss + 0.25*dis_wrong_text_loss + 0.25*dis_wrong_image_loss + 0.5*dis_fake_loss\n\n            dis_fake_logits = models[\"discriminator\"](fake_images, enc_right_texts)\n            gen_loss = criterion(dis_fake_logits, ones)\n\n        dis_score, gen_score = torch.tensor(0), torch.tensor(0)\n\n        total_dis_loss += dis_loss.item(); total_gen_loss += gen_loss.item()\n        total_dis_score += dis_score.item(); total_gen_score += gen_score.item()\n        # print statistics\n        loader.set_postfix_str(template.format(dis_loss, dis_score, gen_loss, gen_score))\n        loader.update()\n        # Clear variable\n        del right_images; del right_texts; del wrong_images; del wrong_texts\n        del enc_right_texts; del enc_wrong_texts; del noise; del fake_images\n        del ones; del zeros; del dis_right_logits; del dis_wrong_text_logits\n        del dis_wrong_image_logits; del dis_fake_logits; del dis_real_loss\n        del dis_wrong_text_loss; del dis_wrong_image_loss; del dis_fake_loss\n        del dis_loss; del gen_loss; del dis_score; del gen_score\n        torch.cuda.empty_cache()\n    template = template.format(total_dis_loss/idx, total_dis_score/idx, total_gen_loss/idx, total_gen_score/idx)\n    loader.write(f\"Validated | {template}\")\n    return total_dis_loss/idx, total_dis_score/idx, total_gen_loss/idx, total_gen_score/idx\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:54:09.236214Z","iopub.execute_input":"2024-10-17T10:54:09.236568Z","iopub.status.idle":"2024-10-17T10:54:09.252815Z","shell.execute_reply.started":"2024-10-17T10:54:09.236531Z","shell.execute_reply":"2024-10-17T10:54:09.251757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train():\n    total_dis_loss, total_gen_loss = 0, 0\n    total_dis_score, total_gen_score = 0, 0\n    template = \"Dis Loss: {:.4f}, Dis Score: {:.4f}, Gen Loss: {:.4f}, Gen Score: {:.4f}\"\n    for key in models:\n        models[key].train()\n    loader = tqdm(train_loader, desc = f\"Training\")\n    for idx, ((right_images, right_texts), (wrong_images, wrong_texts)) in enumerate(loader, start=1):\n        # Execute\n        enc_right_texts = models[\"text_encoder\"](**right_texts).detach()\n        enc_wrong_texts = models[\"text_encoder\"](**wrong_texts).detach()\n\n        noise = torch.randn((len(right_images), 100, 1, 1), device=device)\n        ones = torch.ones((len(right_images)), dtype=torch.long, device=device)\n        zeros = torch.zeros((len(right_images)), dtype=torch.long, device=device)\n        fake_images = models[\"generator\"](noise, enc_right_texts)\n\n        dis_right_logits = models[\"discriminator\"](right_images, enc_right_texts)\n        dis_wrong_text_logits = models[\"discriminator\"](right_images, enc_wrong_texts)\n        dis_wrong_image_logits = models[\"discriminator\"](wrong_images, enc_right_texts)\n        dis_fake_logits = models[\"discriminator\"](fake_images.detach(), enc_right_texts)\n\n        dis_real_loss = criterion(dis_right_logits, ones)\n        dis_wrong_text_loss = criterion(dis_wrong_text_logits, zeros)\n        dis_wrong_image_loss = criterion(dis_wrong_image_logits, zeros)\n        dis_fake_loss = criterion(dis_fake_logits, zeros)\n        dis_loss = dis_real_loss + 0.25*dis_wrong_text_loss + 0.25*dis_wrong_image_loss + 0.5*dis_fake_loss\n\n        optimizers[\"discriminator\"].zero_grad()\n        dis_loss.backward()\n        optimizers[\"discriminator\"].step()\n\n        dis_fake_logits = models[\"discriminator\"](fake_images, enc_right_texts)\n        gen_loss = criterion(dis_fake_logits, ones)\n        optimizers[\"generator\"].zero_grad()\n        gen_loss.backward()\n        optimizers[\"generator\"].step()\n\n        dis_score, gen_score = torch.tensor(0), torch.tensor(0)\n\n        total_dis_loss += dis_loss.item(); total_gen_loss += gen_loss.item()\n        total_dis_score += dis_score.item(); total_gen_score += gen_score.item()\n        # print statistics\n        loader.set_postfix_str(template.format(dis_loss, dis_score, gen_loss, gen_score))\n        loader.update()\n        # Clear variable\n        del right_images; del right_texts; del wrong_images; del wrong_texts\n        del enc_right_texts; del enc_wrong_texts; del noise; del fake_images\n        del ones; del zeros; del dis_right_logits; del dis_wrong_text_logits\n        del dis_wrong_image_logits; del dis_fake_logits; del dis_real_loss\n        del dis_wrong_text_loss; del dis_wrong_image_loss; del dis_fake_loss\n        del dis_loss; del gen_loss; del dis_score; del gen_score\n        torch.cuda.empty_cache()\n    template = template.format(total_dis_loss/idx, total_dis_score/idx, total_gen_loss/idx, total_gen_score/idx)\n    loader.write(f\"Trained | {template}\")\n    for key in schedulers:\n        schedulers[key].step()\n    return total_dis_loss/idx, total_dis_score/idx, total_gen_loss/idx, total_gen_score/idx","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:54:11.126645Z","iopub.execute_input":"2024-10-17T10:54:11.127024Z","iopub.status.idle":"2024-10-17T10:54:11.143892Z","shell.execute_reply.started":"2024-10-17T10:54:11.126987Z","shell.execute_reply":"2024-10-17T10:54:11.142896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = checkpoint.get('train_data', np.empty((0, 4)))\nvalid_data = checkpoint.get('valid_data', np.empty((0, 4)))\nepoch_data = checkpoint.get('epoch_data', [])\n# del checkpoint\ndef plot_training_data(ax, epoch_data, train_data, valid_data):\n    ax[0].clear(); ax[1].clear(); ax[2].clear(); ax[3].clear()\n    ax[0].plot(epoch_data, train_data[:, 0], label = f\"Train dis loss {train_data[-1, 0]:.4f}\")\n    ax[0].plot(epoch_data, valid_data[:, 0], label = f\"Valid dis loss {valid_data[-1, 0]:.4f}\")\n    ax[1].plot(epoch_data, train_data[:, 1], label = f\"Train dis score {train_data[-1, 1]:.4f}\")\n    ax[1].plot(epoch_data, valid_data[:, 1], label = f\"Valid dis score {valid_data[-1, 1]:.4f}\")\n    ax[2].plot(epoch_data, train_data[:, 2], label = f\"Train gen loss {train_data[-1, 2]:.4f}\")\n    ax[2].plot(epoch_data, valid_data[:, 2], label = f\"Valid gen loss {valid_data[-1, 2]:.4f}\")\n    ax[3].plot(epoch_data, train_data[:, 3], label = f\"Train gen score {train_data[-1, 3]:.4f}\")\n    ax[3].plot(epoch_data, valid_data[:, 3], label = f\"Valid gen score {valid_data[-1, 3]:.4f}\")\n    ax[0].legend(); ax[1].legend(); ax[2].legend(); ax[3].legend()\nif epoch_data:\n    fig, ax = plt.subplots(1, 4, figsize=(10*4, 5))\n    plot_training_data(ax, epoch_data, train_data, valid_data)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:54:12.64509Z","iopub.execute_input":"2024-10-17T10:54:12.645444Z","iopub.status.idle":"2024-10-17T10:54:13.461798Z","shell.execute_reply.started":"2024-10-17T10:54:12.645412Z","shell.execute_reply":"2024-10-17T10:54:13.460875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loader = tqdm(range(len(epoch_data), len(epoch_data) + num_epoch * training), desc = \"Epoch\")\nboard = ipywidgets.Output()\nif training: display.display(board)\ngraph = display.display(ipywidgets.widgets.HTML(f\"<b>Training statred: {bool(training)}</b>\"), display_id = True)\nfor i in loader:\n    with board:\n        epoch_data.append(i+1)\n        # Make grid\n        fig, ax = plt.subplots(1, 4, figsize=(10*4, 5))\n        # Close figure\n        plt.close(fig)\n        train_data = np.append(train_data, [train()], axis = 0)\n        valid_data = np.append(valid_data, [valid()], axis = 0)\n        # Visualize\n        plot_training_data(ax, epoch_data, train_data, valid_data)\n        graph.update(fig)\n        # Clear all progress bar with in board widget\n        display.clear_output()\n        graph = display.display(fig, display_id = True)\n    # Save model\n    params = {\n        'models': dict([(key, models[key].state_dict()) for key in models]),\n        'optimizers': dict([(key, optimizers[key].state_dict()) for key in optimizers]),\n        'schedulers': dict([(key, schedulers[key].state_dict()) for key in schedulers]),\n        'train_indices': train_indices,\n        'valid_indices': valid_indices,\n        'train_data': train_data,\n        'valid_data': valid_data,\n        'epoch_data': epoch_data\n    }\n    torch.save(params, \"checkpoint.pth\")\nloader.write(\"Done!\")","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:54:15.84141Z","iopub.execute_input":"2024-10-17T10:54:15.841884Z","iopub.status.idle":"2024-10-17T18:50:18.672914Z","shell.execute_reply.started":"2024-10-17T10:54:15.841834Z","shell.execute_reply":"2024-10-17T18:50:18.67178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx = np.random.randint(len(train_dataset))\n(image, text), _ = train_dataset[idx]\nparams = {}\nfor key in text:\n    params[key] = torch.tensor(text[key], device=device)[None]\nfor key in models:\n    models[key].eval()\nenc_text = models[\"text_encoder\"](**params)\nnoise = torch.randn((1, 100, 1, 1), device=device)\ngen_image = models[\"generator\"](noise, enc_text).detach().squeeze().cpu()\n\n# Retrieve the text from the \"Right\" image and use it, but don't plot the \"Right\" image\nplt.figure(figsize=(5, 5))\nvisualize(gen_image, text, train_dataset.tokenizer, plt.gca(), \"Generated\")  # Passing the text from the \"Right\" image\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T19:27:25.294394Z","iopub.execute_input":"2024-10-17T19:27:25.294788Z","iopub.status.idle":"2024-10-17T19:27:25.561199Z","shell.execute_reply.started":"2024-10-17T19:27:25.294753Z","shell.execute_reply":"2024-10-17T19:27:25.560152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset","metadata":{"execution":{"iopub.status.busy":"2024-10-17T19:27:27.294093Z","iopub.execute_input":"2024-10-17T19:27:27.294498Z","iopub.status.idle":"2024-10-17T19:27:27.301121Z","shell.execute_reply.started":"2024-10-17T19:27:27.294464Z","shell.execute_reply":"2024-10-17T19:27:27.299981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {}\ntxt=input()\ntext = np.array(txt).item().strip()\ntokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\ninput_ids = tokenizer.encode(text)\ntoken_type_ids = [0] * (len(input_ids) - 1) + [1]\nattention_mask = [1] * len(token_type_ids)\ntext = {\n    \"input_ids\": input_ids,\n    \"token_type_ids\": token_type_ids,\n    \"attention_mask\": attention_mask\n}\nfor key in models:\n    models[key].eval()\nfor key in text:\n#     print(key)\n    params[key] = torch.tensor(text[key], device=device)[None]\nenc_text = models[\"text_encoder\"](**params)\nnoise = torch.randn((1, 100, 1, 1), device=device)\ngen_image = models[\"generator\"](noise, enc_text).detach().squeeze().cpu()\n\n_, ax = plt.subplots(1, 1, figsize=(5, 5))\nvisualize(gen_image, text, train_dataset.tokenizer, ax, title = \"Generated Image\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-17T19:27:49.918489Z","iopub.execute_input":"2024-10-17T19:27:49.918855Z","iopub.status.idle":"2024-10-17T19:28:04.326406Z","shell.execute_reply.started":"2024-10-17T19:27:49.918819Z","shell.execute_reply":"2024-10-17T19:28:04.325384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}