{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9660712,"sourceType":"datasetVersion","datasetId":5902256},{"sourceId":9660871,"sourceType":"datasetVersion","datasetId":5902370}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\nwith open('/kaggle/input/dataset/ordered_embeddings_dict.pkl', 'rb') as pickle_file:\n    ordered_embeddings_dict = pickle.load(pickle_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T00:33:38.459833Z","iopub.execute_input":"2024-10-19T00:33:38.460663Z","iopub.status.idle":"2024-10-19T00:33:41.489707Z","shell.execute_reply.started":"2024-10-19T00:33:38.460622Z","shell.execute_reply":"2024-10-19T00:33:41.486934Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  return torch.load(io.BytesIO(b))\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pickle\nwith open('/kaggle/input/dataset/ordered_image_tensor_dict (1).pkl', 'rb') as pickle_file:\n    ordered_image_tensor_dict = pickle.load(pickle_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T00:33:41.491308Z","iopub.execute_input":"2024-10-19T00:33:41.491655Z","iopub.status.idle":"2024-10-19T00:34:28.111629Z","shell.execute_reply.started":"2024-10-19T00:33:41.491620Z","shell.execute_reply":"2024-10-19T00:34:28.110631Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass GenerateC(nn.Module):\n    def __init__(self):\n        super(GenerateC, self).__init__()\n        \n    def forward(self, x):\n        mean = x[:, :128]  \n#         print(\"mean:\" , mean.shape)\n        log_sigma = x[:, 128:]\n#         print(\"logsigma:\", log_sigma.shape)\n        stddev = torch.exp(log_sigma)\n#         print(\"stdev shape:\",stddev.shape)\n        epsilon = torch.randn(mean.shape[0], mean.shape[1], device=mean.device)\n#         print(\"epsilon shape:\",epsilon.shape)\n        c = stddev * epsilon + mean\n        return c\n\nclass ConditionalAugmentation(nn.Module):\n    def __init__(self):\n        super(ConditionalAugmentation, self).__init__()\n        self.fc = nn.Linear(768, 256)  # Adjusted to 768 input and 256 output\n        self.lrelu = nn.LeakyReLU(0.2)\n        \n    def forward(self, x):\n        x = self.fc(x)\n        x = self.lrelu(x)\n        return x\n\nclass EmbeddingCompressor(nn.Module):\n    def __init__(self):\n        super(EmbeddingCompressor, self).__init__()\n        self.fc = nn.Linear(768, 256)  # Adjusted to 768 input and 256 output\n        self.relu = nn.ReLU()\n        \n    def forward(self, x):\n        x = self.fc(x)\n        x = self.relu(x)\n        return x\n\nclass Stage1Generator(nn.Module):\n    def __init__(self):\n        super(Stage1Generator, self).__init__()\n        self.fc1 = nn.Linear(768, 256)  # Adjusted to 768 input and 256 output\n        self.lrelu = nn.LeakyReLU(0.2)\n        self.generate_c = GenerateC()\n        \n        self.fc2 = nn.Linear(128 + 100, 128 * 8 * 4 * 4)  # Adjusted to 256 + 100\n        self.relu = nn.ReLU()\n        self.reshape = nn.Unflatten(1, (128 * 8, 4, 4))\n        \n        self.upconv1 = nn.Sequential(\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128 * 8, 512, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(512),\n            nn.ReLU()\n        )\n        \n        self.upconv2 = nn.Sequential(\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(512, 256, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.ReLU()\n        )\n        \n        self.upconv3 = nn.Sequential(\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(256, 128, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.ReLU()\n        )\n        \n        self.upconv4 = nn.Sequential(\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 64, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU()\n        )\n        \n        self.final_conv = nn.Conv2d(64, 3, kernel_size=3, padding=1, bias=False)\n        self.tanh = nn.Tanh()\n        \n    def forward(self, x1, x2):\n        x1= x1.squeeze(1)\n        x1=self.fc1(x1)\n        mean_logsigma = self.lrelu(x1)\n#         print(\"mean_logsigma shape:\", mean_logsigma.shape)\n        c = self.generate_c(mean_logsigma)\n#         print(\"c shape\",c.shape)\n        gen_input = torch.cat([c, x2], dim=1)\n#         print(\"shape after concatenate:\", gen_input.shape)\n        \n        x = self.fc2(gen_input)\n        x = self.relu(x)\n        x = self.reshape(x)\n        \n        x = self.upconv1(x)\n        x = self.upconv2(x)\n        x = self.upconv3(x)\n        x = self.upconv4(x)\n        x = self.final_conv(x)\n        x = self.tanh(x)\n        \n        return x, mean_logsigma\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n        self.bn1 = nn.BatchNorm2d(in_channels)\n        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(in_channels)\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = F.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out += residual\n        out = F.relu(out)\n        return out\n\nclass JointBlock(nn.Module):\n    def forward(self, c, x):\n        c = c.unsqueeze(2).unsqueeze(3)\n        c = c.expand(-1, -1, x.size(2), x.size(3))\n        return torch.cat([c, x], dim=1)\n\nclass Stage2Generator(nn.Module):\n    def __init__(self):\n        super(Stage2Generator, self).__init__()\n        \n        # CA Augmentation Network\n        self.fc = nn.Linear(768, 256)\n        self.leaky_relu = nn.LeakyReLU(0.2)\n        \n        # Image Encoder\n        self.conv1 = nn.Conv2d(3, 128, kernel_size=3,stride=1, padding=1)\n        self.conv2 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n        self.conv3 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)\n        \n        self.bn2 = nn.BatchNorm2d(256)\n        self.bn3 = nn.BatchNorm2d(512)\n        self.reduce_channels = nn.Conv2d(768, 512, kernel_size=1)\n        \n        # Residual Blocks\n        self.residual_block1 = ResidualBlock(512)\n        self.residual_block2 = ResidualBlock(512)\n        self.residual_block3 = ResidualBlock(512)\n        self.residual_block4 = ResidualBlock(512)\n        \n        # Upsampling Layers\n        self.upsample1 = nn.Upsample(scale_factor=2)\n        self.conv_up1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.bn_up1 = nn.BatchNorm2d(512)\n        \n        self.upsample2 = nn.Upsample(scale_factor=2)\n        self.conv_up2 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n        self.bn_up2 = nn.BatchNorm2d(256)\n        \n        self.upsample3 = nn.Upsample(scale_factor=2)\n        self.conv_up3 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n        self.bn_up3 = nn.BatchNorm2d(128)\n        \n        self.upsample4 = nn.Upsample(scale_factor=2)\n        self.conv_up4 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n        self.bn_up4 = nn.BatchNorm2d(64)\n        \n        # Final layer\n        self.final_conv = nn.Conv2d(64, 3, kernel_size=3, padding=1)\n        self.tanh = nn.Tanh()\n        \n    def forward(self, z, lr_img):\n        # CA Augmentation\n        z = z.squeeze(1)\n        c = self.fc(z)\n        c = self.leaky_relu(c)\n        \n        # Image Encoder\n        x = F.pad(lr_img, (1, 1, 1, 1))\n        x = F.relu(self.conv1(x))\n        \n        x = F.pad(x, (1, 1, 1, 1))\n        x = F.relu(self.bn2(self.conv2(x)))\n        \n        x = F.pad(x, (1, 1, 1, 1))\n        x = F.relu(self.bn3(self.conv3(x)))\n        \n        # Joint block\n        joint_block = JointBlock()\n        c_code = joint_block(c, x)\n        c_code = self.reduce_channels(c_code)\n        # print(c_code.shape)\n        \n        # Residual blocks\n        x = self.residual_block1(c_code)\n        x = self.residual_block2(x)\n        x = self.residual_block3(x)\n        x = self.residual_block4(x)\n        \n        # Upsampling blocks\n        x = self.upsample1(x)\n        x = F.relu(self.bn_up1(self.conv_up1(x)))\n        \n        x = self.upsample2(x)\n        x = F.relu(self.bn_up2(self.conv_up2(x)))\n        \n        x = self.upsample3(x)\n        x = F.relu(self.bn_up3(self.conv_up3(x)))\n        \n        x = self.upsample4(x)\n        x = F.relu(self.bn_up4(self.conv_up4(x)))\n        \n        # Final output\n        x = self.final_conv(x)\n        x = self.tanh(x)\n        \n        return x, c\n\n\nclass Stage2Discriminator(nn.Module):\n    def __init__(self):\n        super(Stage2Discriminator, self).__init__()\n        self.fc_2 = nn.Linear(768, 128)\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1)\n        self.conv2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n        self.conv3 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n        self.conv4 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)\n        self.conv5 = nn.Conv2d(512, 1024, kernel_size=4, stride=2, padding=1)\n        self.conv6 = nn.Conv2d(1024, 2048, kernel_size=4, stride=2, padding=1)\n        \n        self.bn2 = nn.BatchNorm2d(128)\n        self.bn3 = nn.BatchNorm2d(256)\n        self.bn4 = nn.BatchNorm2d(512)\n        self.bn5 = nn.BatchNorm2d(1024)\n        self.bn6 = nn.BatchNorm2d(2048)\n        \n        self.conv7 = nn.Conv2d(2048, 1024, kernel_size=1, stride=1, padding=0)\n        self.bn7 = nn.BatchNorm2d(1024)\n        \n        self.conv8 = nn.Conv2d(1024, 512, kernel_size=1, stride=1, padding=0)\n        self.bn8 = nn.BatchNorm2d(512)\n        \n        self.conv9 = nn.Conv2d(640, 128, kernel_size=1, stride=1, padding=0)\n        self.bn9 = nn.BatchNorm2d(128)\n        \n        self.conv10 = nn.Conv2d(128, 512, kernel_size=3, stride=1, padding=1)\n        self.bn10 = nn.BatchNorm2d(512)\n        \n        self.conv_final = nn.Conv2d(512, 1, kernel_size=4, stride=1)\n        \n    def forward(self, img, embd):\n        \n        x = F.leaky_relu(self.conv1(img), 0.2)\n        x = F.leaky_relu(self.bn2(self.conv2(x)), 0.2)\n        x = F.leaky_relu(self.bn3(self.conv3(x)), 0.2)\n        x = F.leaky_relu(self.bn4(self.conv4(x)), 0.2)\n        x = F.leaky_relu(self.bn5(self.conv5(x)), 0.2)\n        x = F.leaky_relu(self.bn6(self.conv6(x)), 0.2)\n        \n        x = F.leaky_relu(self.bn7(self.conv7(x)), 0.2)\n        x = F.leaky_relu(self.bn8(self.conv8(x)), 0.2)\n        \n        # Joint with c_code\n        # c_code= c_code.squeeze(1)\n        c_code = self.fc_2(embd)\n        c_code= c_code.squeeze(1)\n        c_code = c_code.unsqueeze(2).unsqueeze(3)\n        c_code = c_code.expand(c_code.size(0), c_code.size(1), x.size(2), x.size(3))\n        x = torch.cat([x, c_code], dim=1)\n        \n        x = F.leaky_relu(self.bn9(self.conv9(x)), 0.2)\n        x = F.leaky_relu(self.bn10(self.conv10(x)), 0.2)\n        \n        validity = self.conv_final(x)\n        return torch.sigmoid(validity)\n\nclass AdversarialModel(nn.Module):\n    def __init__(self, gen_model1, gen_model2, dis_model):\n        \"\"\"\n        Initialize adversarial model.\n        \"\"\"\n        super(AdversarialModel, self).__init__()\n        self.gen_model1 = gen_model1\n        self.gen_model2 = gen_model2\n        self.dis_model = dis_model\n        \n        # Freeze gen_model1 and dis_model parameters (set them to eval mode)\n        self.gen_model1.eval()\n        self.dis_model.eval()\n        \n\n    def forward(self, embeddings_input, noise_input):\n\n        # Pass through gen_model1 to get low-resolution images and mean_logsigma1\n        with torch.no_grad():  # Freeze the first generator\n            lr_images, mean_logsigma1 = self.gen_model1(embeddings_input, noise_input)\n        \n        # Pass low-resolution images through gen_model2 to get high-resolution images and mean_logsigma2\n        hr_images, mean_logsigma2 = self.gen_model2(embeddings_input, lr_images)\n        \n        # Pass high-resolution images and compressed embeddings through the discriminator\n        valid = self.dis_model(hr_images, embeddings_input)\n        \n        return valid, mean_logsigma2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T00:34:28.114196Z","iopub.execute_input":"2024-10-19T00:34:28.114777Z","iopub.status.idle":"2024-10-19T00:34:28.174556Z","shell.execute_reply.started":"2024-10-19T00:34:28.114732Z","shell.execute_reply":"2024-10-19T00:34:28.173749Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def KL_loss(y_pred):\n    # Extract mean and log_sigma from y_pred\n    mean = y_pred[:, :128]\n    log_sigma = y_pred[:, 128:]\n    loss = -log_sigma + 0.5 * (-1 + torch.exp(2. * log_sigma) + mean**2)\n\n    loss = loss.mean()\n    \n    return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T00:34:28.176621Z","iopub.execute_input":"2024-10-19T00:34:28.176921Z","iopub.status.idle":"2024-10-19T00:34:28.200524Z","shell.execute_reply.started":"2024-10-19T00:34:28.176890Z","shell.execute_reply":"2024-10-19T00:34:28.199785Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\ndef save_rgb_img(img, path):\n\n    fig = plt.figure()\n    ax = fig.add_subplot(1, 1, 1)\n    if isinstance(img, torch.Tensor):\n        img = img.detach().permute(1, 2, 0).cpu().numpy()  \n    \n    ax.imshow(img)\n    ax.axis(\"off\")\n    ax.set_title(\"Image\")\n\n    plt.savefig(path)\n    plt.close()\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T00:34:28.201529Z","iopub.execute_input":"2024-10-19T00:34:28.201807Z","iopub.status.idle":"2024-10-19T00:34:28.215589Z","shell.execute_reply.started":"2024-10-19T00:34:28.201778Z","shell.execute_reply":"2024-10-19T00:34:28.214864Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from torch.utils.tensorboard import SummaryWriter\n\ndef write_log(writer, name, loss, global_step):\n\n    writer.add_scalar(name, loss, global_step)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T00:34:28.218657Z","iopub.execute_input":"2024-10-19T00:34:28.218982Z","iopub.status.idle":"2024-10-19T00:34:28.225985Z","shell.execute_reply.started":"2024-10-19T00:34:28.218948Z","shell.execute_reply":"2024-10-19T00:34:28.225303Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def plot_generated_images(images, epoch, n_images=8):\n\n    fig, axes = plt.subplots(1, n_images, figsize=(n_images * 2, 2))  \n    for i in range(n_images):\n        ax = axes[i]\n        img = images[i].detach().permute(1, 2, 0).cpu().numpy()  \n        ax.imshow(img)\n        ax.axis(\"off\")  \n    plt.suptitle(f'Generated Images at Epoch {epoch}')  \n    plt.show()  # Display the plot","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T00:34:28.227310Z","iopub.execute_input":"2024-10-19T00:34:28.227646Z","iopub.status.idle":"2024-10-19T00:34:28.238609Z","shell.execute_reply.started":"2024-10-19T00:34:28.227612Z","shell.execute_reply":"2024-10-19T00:34:28.237790Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import torch\nimport time\nimport gc\nfrom pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo\n\ndef clear_gpu_memory():\n    torch.cuda.empty_cache()\n    gc.collect()\n    del variables\n\ndef wait_until_enough_gpu_memory(min_memory_available, max_retries=10, sleep_time=5):\n    nvmlInit()\n    handle = nvmlDeviceGetHandleByIndex(torch.cuda.current_device())\n\n    for _ in range(max_retries):\n        info = nvmlDeviceGetMemoryInfo(handle)\n        if info.free >= min_memory_available:\n            break\n        print(f\"Waiting for {min_memory_available} bytes of free GPU memory. Retrying in {sleep_time} seconds...\")\n        time.sleep(sleep_time)\n    else:\n        raise RuntimeError(f\"Failed to acquire {min_memory_available} bytes of free GPU memory after {max_retries} retries.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T00:34:28.239932Z","iopub.execute_input":"2024-10-19T00:34:28.240532Z","iopub.status.idle":"2024-10-19T00:34:28.250750Z","shell.execute_reply.started":"2024-10-19T00:34:28.240491Z","shell.execute_reply":"2024-10-19T00:34:28.250029Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport numpy as np\nimport os\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm import tqdm\nimport time\n\n# Hyperparameters and setup\nbatch_size = 8\nz_dim = 100\nstage1_generator_lr = 0.0002\nstage1_discriminator_lr = 0.0002\nepochs = 250\ncondition_dim = 128\ntorch.cuda.empty_cache()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Create a tensorboard writer\nwriter = SummaryWriter(log_dir=\"logs/\".format(time.time()))\n\n# Optimizers\n\n\n# Loss functions\nbce_loss = nn.BCELoss().to(device)\n\nstage2_gen = Stage2Generator().to(device)\nstage2_dis = Stage2Discriminator().to(device)\nembedding_compressor_model = EmbeddingCompressor().to(device)\nstage1_gen = Stage1Generator().to(device)\nstage1_gen.load_state_dict(torch.load(\"/kaggle/input/weight-2/generator_epoch_40.pth\"))  # Load the weights\nstage1_gen.eval()  # Set to evaluation mode\nadversarial_model = AdversarialModel(stage1_gen,stage2_gen, stage2_dis).to(device)\n\ndis_optimizer = optim.Adam(stage2_dis.parameters(), lr=stage1_discriminator_lr, betas=(0.5, 0.999))\ngen_optimizer = optim.Adam(stage2_gen.parameters(), lr=stage1_generator_lr, betas=(0.5, 0.999))\n\n# Labels\nreal_labels = torch.full((batch_size, 1), 0.9, dtype=torch.float, device=device)\nfake_labels = torch.full((batch_size, 1), 0.1, dtype=torch.float, device=device)\n\n# Data preparation (assume dictionaries are in order)\nimage_keys = list(ordered_image_tensor_dict.keys())\n\n# Training loop\nfor epoch in tqdm(range(epochs)):\n    print(f\"Epoch {epoch + 1}/{epochs}\")\n\n    gen_losses = []\n    dis_losses = []\n\n    num_batches = len(ordered_image_tensor_dict) // batch_size\n    for index in tqdm(range(num_batches)):\n        # Select batch\n        batch_keys = image_keys[index * batch_size:(index + 1) * batch_size]\n        real_images = torch.stack([ordered_image_tensor_dict[key] for key in batch_keys]).to(device)\n        embeddings = torch.stack([ordered_embeddings_dict[key] for key in batch_keys]).to(device)\n\n        # Normalize real images\n        # real_images = (real_images - 127.5) / 127.5\n\n        # Generate noise and fake images\n        z_noise = torch.randn(batch_size, z_dim, device=device)\n        lr_fake_images, _ = stage1_gen(embeddings, z_noise)\n        hr_fake_images, _ = stage2_gen(embeddings, lr_fake_images)\n\n        # Train the discriminator\n        stage2_dis.zero_grad()\n\n        # Train with real images\n        dis_real_output = stage2_dis(real_images, embeddings).view(-1)  # Flatten the output\n        dis_loss_real = bce_loss(dis_real_output, real_labels.view(-1))  # Flatten the labels\n\n# Train with fake images\n        dis_fake_output = stage2_dis(hr_fake_images.detach(), embeddings).view(-1)  # Flatten the output\n        dis_loss_fake = bce_loss(dis_fake_output, fake_labels.view(-1))\n\n        wrong_output = stage2_dis(real_images[:(batch_size - 1)], embeddings[1:]).view(-1)  # Flatten the output\n        wrong_loss = bce_loss(wrong_output, fake_labels[1:].view(-1).to(device))  \n        \n        # Combine losses\n        d_loss = 0.5 * (dis_loss_real + 0.5 * (wrong_loss + dis_loss_fake))\n        d_loss.backward()\n        dis_optimizer.step()\n\n        # Train the generator\n        stage2_gen.zero_grad()\n\n        # Adversarial loss and KL loss\n        valid, mean_logsigma2 = adversarial_model(embeddings, z_noise)\n        g_loss_adv = bce_loss(valid.squeeze(), real_labels.view(-1))\n        g_loss_kl = KL_loss(mean_logsigma2)\n        g_loss = g_loss_adv + 2 * g_loss_kl\n\n        g_loss.backward()\n        gen_optimizer.step()\n\n        # Record losses\n        gen_losses.append(g_loss.item())\n        dis_losses.append(d_loss.item())\n    \n        if index % 50 == 0:\n            print(f'Epoch [{epoch}/{epochs}] Step [{index}] Discriminator Loss: {d_loss.item()} Generator Loss: {g_loss.item()}')\n\n    writer.add_scalar('Discriminator Loss', np.mean(dis_losses), epoch)\n    writer.add_scalar('Generator Loss', np.mean(gen_losses), epoch)\n            \n    if epoch % 10 == 0:\n        torch.save(stage2_gen.state_dict(), f'generator_epoch_{epoch}.pth')\n        torch.save(stage2_dis.state_dict(), f'discriminator_epoch_{epoch}.pth')\n    # Print average losses per epoch\n    if epoch % 10 == 0:\n        img_path = os.path.join(output_dir, f'generated_img_epoch_{epoch}.png')\n        save_rgb_img(hr_fake_images[0], img_path)  # Save the first image in the batch for visualization\n        \n        # Plot generated images\n        plot_generated_images(fake_images, epoch)  \n    # Save generated images\n    if epoch % 2 == 0:\n        with torch.no_grad():\n            z_noise2 = torch.randn(batch_size, z_dim, device=device)\n            lr_fake_images, _ = stage1_gen(embeddings, z_noise2)\n            hr_fake_images, _ = stage2_gen(embeddings, lr_fake_images)\n            save_images(hr_fake_images, epoch, batch_size)\n\n# Save the models after training\ntorch.save(stage2_gen.state_dict(), \"stage2_gen.pth\")\ntorch.save(stage2_dis.state_dict(), \"stage2_dis.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T00:40:02.796873Z","iopub.execute_input":"2024-10-19T00:40:02.797516Z","iopub.status.idle":"2024-10-19T00:40:04.988093Z","shell.execute_reply.started":"2024-10-19T00:40:02.797474Z","shell.execute_reply":"2024-10-19T00:40:04.986518Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/1844541522.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  stage1_gen.load_state_dict(torch.load(\"/kaggle/input/weight-2/generator_epoch_40.pth\"))  # Load the weights\n  0%|          | 0/250 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 1/250\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/102 [00:00<?, ?it/s]\u001b[A\n  0%|          | 0/250 [00:00<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 67\u001b[0m\n\u001b[1;32m     65\u001b[0m z_noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, z_dim, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     66\u001b[0m lr_fake_images, _ \u001b[38;5;241m=\u001b[39m stage1_gen(embeddings, z_noise)\n\u001b[0;32m---> 67\u001b[0m hr_fake_images, _ \u001b[38;5;241m=\u001b[39m \u001b[43mstage2_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_fake_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Train the discriminator\u001b[39;00m\n\u001b[1;32m     70\u001b[0m stage2_dis\u001b[38;5;241m.\u001b[39mzero_grad()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[6], line 216\u001b[0m, in \u001b[0;36mStage2Generator.forward\u001b[0;34m(self, z, lr_img)\u001b[0m\n\u001b[1;32m    213\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn_up3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_up3(x)))\n\u001b[1;32m    215\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsample4(x)\n\u001b[0;32m--> 216\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn_up4(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_up4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Final output\u001b[39;00m\n\u001b[1;32m    219\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_conv(x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.27 GiB. GPU 0 has a total capacity of 14.74 GiB of which 520.12 MiB is free. Process 2412 has 14.23 GiB memory in use. Of the allocated memory 12.36 GiB is allocated by PyTorch, and 1.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 1.27 GiB. GPU 0 has a total capacity of 14.74 GiB of which 520.12 MiB is free. Process 2412 has 14.23 GiB memory in use. Of the allocated memory 12.36 GiB is allocated by PyTorch, and 1.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}